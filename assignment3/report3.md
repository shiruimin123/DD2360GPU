**contribute:** Bowen Tian did exercise 1, Ruimin Shi did exercise 2.

# Assignment III:

## Exercise 1

### 1. Describe all optimizations you tried regardless of whether you committed to them or abandoned them and whether they improved or hurt performance. 

We declare a shared memory (Bins) to store a local histogram sinccce shared memory is much faster than global memory and can be used for efficient inter-thread communication within a block. And we updated the histogram bins with atomic operation(atomicAdd). This ensures that multiple threads can safely increment the same bin without conflicts. 

In addition to this, I tried doing parallel initialization. My original code is shown below and the kernal execution time is: **0.000318 seconds**(When the inputLength is set as 100000)

Then we tried to initialize the shared memory in parallel. The code is shown below and the kernal execution time is: **0.000331 seconds**(When the inputLength is set as 100000)

After that, we added the shared memory values to the global memory in parallel, and the kernal execution time is: **0.000059 seconds**(When the inputLength is set as 100000)

Finally we changed the number of threads per block from 256 to 1024, the kernal execution time is: **0.000046 seconds**(When the inputLength is set as 100000).

### 2. Which optimizations you chose in the end and why? 
We set the shared memory to zero during initialization phase and add the shared memory value to global memory in parallel, also we increase the number of threads per block. The detailed information can be refered in the last question. 
### 3. How many global memory reads are being performed by your kernel? 
In the histogram kernal, each thread reads one element from the global memory and updates the corresponding bin in the shared memory. In this design, I set the inputLength as 100_000. So the time of global memory read should be 100_000.

While the convert_kernel does not perform additional global memory reads since it only modifies the contents of the global memory bins based on the calculated histogram.
### 4. How many atomic operations are being performed by your kernel? 
There are two atomic operation for one thread, so the number of atomic operations performed by kernal should be: 200_000.
### 5. How much shared memory is used in your code?
The number of shared memory should be: 
$$[number_of_block] * [number_of_bins] * 4byte $$
So the result is : 
$$[(100000 + 1024 - 1)/1024] * 4096 * 4 = 1622016 $$
### 6. How would the value distribution of the input array affect the contention among threads? For instance, what contentions would you expect if every element in the array has the same value?
### 7. Plot a histogram generated by your code and specify your input length, thread block and grid.
### 8. For a input array of 1024 elements, profile with Nvidia Nsight and report Shared Memory Configuration Size and Achieved Occupancy. Did Nvsight report any potential performance issues?

## Exercise 2
### 1. Describe the environment you used, what changes you made to the Makefile, and how you ran the simulation.
### 2. Describe your design of the GPU implementation of mover_PC() briefly. 
### 3. Compare the output of both CPU and GPU implementation to guarantee that your GPU implementations produce correct answers.
### 4. Compare the execution time of your GPU implementation with its CPU version.


