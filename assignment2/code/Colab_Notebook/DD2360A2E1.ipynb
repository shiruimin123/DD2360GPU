{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Gab_B2_qQhL",
        "outputId": "2ea78dc9-9094-440d-a088-f4f4af5daed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ./drive/MyDrive/DD2360/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4QvF61CKR3D",
        "outputId": "51763f9d-bb6c-4d75-aa78-cf82e43c88be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: './drive/MyDrive/DD2360/'\n",
            "/content/drive/MyDrive/DD2360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 lab2exercise1.cu -o exercise1.out"
      ],
      "metadata": {
        "id": "BP4SgogRKeov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./exercise1.out 131070"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAuGfPPKLwKb",
        "outputId": "3acaf40c-edf1-4447-a4bf-93e882ce4e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 131070\n",
            "Data copy from host to device: 0.000947 seconds\n",
            "Kernel Execution Time: 0.000089 seconds\n",
            "Data copy from device to host: 0.000871 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/cuda-11/bin/nv-nsight-cu-cli ./exercise1.out 131070"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnkLqmF8N3fZ",
        "outputId": "3c7a5b20-cba3-4ba6-8204-e1c10c05124d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 131070\n",
            "==PROF== Connected to process 17444 (/content/drive/MyDrive/DD2360/exercise1.out)\n",
            "Data copy from host to device: 0.000908 seconds\n",
            "==PROF== Profiling \"vecAdd\" - 0: 0%....50%....100% - 8 passes\n",
            "Kernel Execution Time: 0.299340 seconds\n",
            "Data copy from device to host: 0.000965 seconds\n",
            "==PROF== Disconnected from process 17444\n",
            "[17444] exercise1.out@127.0.0.1\n",
            "  vecAdd(double *, double *, double *, int), 2023-Nov-25 20:34:19, Context 1, Stream 7\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           4.47\n",
            "    SM Frequency                                                             cycle/usecond                         521.54\n",
            "    Elapsed Cycles                                                                   cycle                          6,460\n",
            "    Memory [%]                                                                           %                          64.14\n",
            "    DRAM Throughput                                                                      %                          64.14\n",
            "    Duration                                                                       usecond                          12.38\n",
            "    L1/TEX Cache Throughput                                                              %                          32.05\n",
            "    L2 Cache Throughput                                                                  %                          32.92\n",
            "    SM Active Cycles                                                                 cycle                       5,112.32\n",
            "    Compute (SM) [%]                                                                     %                          25.37\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        128\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       1,024\n",
            "    Registers Per Thread                                                   register/thread                             16\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                              byte/block                              0\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        131,072\n",
            "    Waves Per SM                                                                                                     3.20\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    \n",
            "          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       \n",
            "          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 64 thread blocks.   \n",
            "          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   \n",
            "          up to 25.0% of the total kernel runtime with a lower occupancy of 22.0%. Try launching a grid with no         \n",
            "          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  \n",
            "          a grid. See the Hardware Model                                                                                \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             16\n",
            "    Block Limit Registers                                                            block                             32\n",
            "    Block Limit Shared Mem                                                           block                             16\n",
            "    Block Limit Warps                                                                block                              8\n",
            "    Theoretical Active Warps per SM                                                   warp                             32\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          78.03\n",
            "    Achieved Active Warps Per SM                                                      warp                          24.97\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (78.0%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n"
          ]
        }
      ]
    }
  ]
}